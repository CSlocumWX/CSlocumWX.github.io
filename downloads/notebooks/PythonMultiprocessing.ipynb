{
 "metadata": {
  "name": "",
  "signature": "sha256:1770937a7fbabe124ece07ab54274d24a923afc6651b208d5c5232fdd0474d23"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Multiprocessing: Task parallelism for the masses"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>*This notebook originally appeared as a [post](https://cslocumwx.github.io/blog/2015/02/23/python-multiprocessing/) on the blog [Atmospheric Algorithm Anthics](https://cslocumwx.github.io). <span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Multiprocessing: Task parallelism for the masses</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://schubert.atmos.colostate.edu/~cslocum/\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">Chris Slocum</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-nd/3.0/deed.en_US\">Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License</a>.*</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this post, we will discuss one of the two recognized types of parallelism (**task** and **data**).\n",
      "- *Task parallelism* (also known as function parallelism or control parallelism) as the name suggest distributes work across multiple processors. Task parallelism distributes processes across the processors or nodes. This is different than the second type of parallelism, data parallelism.\n",
      "- In *data parallelism*, it is the information that is distributed across processors. An example of data parallelism is taking a summation of values in a data array. In data parallelism, groups of numbers are sent to each core to do the addition. Those values are then added on a smaller group of cores until a single number is computed.\n",
      "\n",
      "Task parallelism sends a unit of work to a processor. In our summation example, a single processor is responsible for calculating the sum of a single data array while maybe another processor does that task with a completely seperate array.\n",
      "\n",
      "If you are interested in data parallelism, this post is not for you. A lot of work has been done on the topic and enabled within the IPython environment. I would suggest checking out [Using MPI with IPython](http://ipython.org/ipython-doc/dev/parallel/parallel_mpi.html)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## OK, why do I care?\n",
      "The current belief is that human time costs more than computer time. If you can get more done using most of the resources available for your machine, then the work gets done faster and you the human can interpret the result. Alright, so if you are saying to yourself that this sounds awesome but I'm not a computer scientist, you don't need to be. The great contributors to Python have done that work for you.\n",
      "\n",
      "## What is the difference between multithreading and multiprocessing?\n",
      "The multithreading paradigm allows for multiple requests to be managed by the same program without running multiple copies. Multithreading is typically used on systems with a single CPU. In multiprocessing, multiple processes or jobs can be run and managed by the CPU or a single program. This is how you are able to have a browser running in addition to a word processor. We are going to deal with the latter. If you are interested in multithreading, there are a handful of tutorials available. \n",
      "\n",
      "## Ok, you've talked a lot about what we aren't covering!!! Let's get started!\n",
      "Yes, let's get back to multiprocessing! Python's `multiprocessing` library has a number of powerful process spawning features which completely side-step issues associated with multithreading. As a result, the `multiprocessing` package within the Python standard library can be used on virtually any operating system. Setting up multiprocessing is actually extremely easy!\n",
      "\n",
      "To use the `multiprocessing` package, we must define a 'unit of work.' This 'unit of work' contains everything we want done in a single task. Lets jump into an example."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>*IPython has some issues spawning subprocesses. To avoid these issues, we will save code to a file then run that file within the notebook.*</small>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file multiproc.py\n",
      "\n",
      "import multiprocessing  # the module we will be using for multiprocessing\n",
      "\n",
      "def work(number):\n",
      "    \"\"\"\n",
      "    Multiprocessing work\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    number : integer\n",
      "        unit of work number\n",
      "    \"\"\"\n",
      "    \n",
      "    print \"Unit of work number %d\" % number  # simply print the worker's number\n",
      "    \n",
      "if __name__ == \"__main__\":  # Allows for the safe importing of the main module\n",
      "    print(\"There are %d CPUs on this machine\" % multiprocessing.cpu_count())\n",
      "    number_processes = 2\n",
      "    pool = multiprocessing.Pool(number_processes)\n",
      "    total_tasks = 16\n",
      "    tasks = range(total_tasks)\n",
      "    results = pool.map_async(work, tasks)\n",
      "    pool.close()\n",
      "    pool.join()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting multiproc.py\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>*Use %%bash for Unix machines and %%cmd for Windows machines*</small>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "python multiproc.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 8 CPUs on this machine\n",
        "Unit of work number 2\n",
        "Unit of work number 3\n",
        "Unit of work number 6\n",
        "Unit of work number 7\n",
        "Unit of work number 10\n",
        "Unit of work number 11\n",
        "Unit of work number 14\n",
        "Unit of work number 15\n",
        "Unit of work number 0\n",
        "Unit of work number 1\n",
        "Unit of work number 4\n",
        "Unit of work number 5\n",
        "Unit of work number 8\n",
        "Unit of work number 9\n",
        "Unit of work number 12\n",
        "Unit of work number 13\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Discussion line by line\n",
      "\n",
      "Lets skip down to the the line\n",
      "```python\n",
      "if __name__ == \"__main__\":\n",
      "```\n",
      "For those of you not familiar with this line, it tells the Python interpretor to only run this section of code when the script is not imported into another piece of code. You can think about it this way\n",
      "```python\n",
      "if __name__ == \"__main__\":\n",
      "\tprint('The program executed by itself')\n",
      "else:\n",
      "\tprint('The program has been imported from another module')\n",
      "```\n",
      "The Python documentation says that this is required when using the `multiprocessing` module in order to allow for \"safe\" importing of the `main` module.\n",
      "\n",
      "```python\n",
      "multiprocessing.cpu_count()\n",
      "```\n",
      "The line above should be self-explanatory. However, it is important we still understand it. `cpu_count()` essentially tells us the theoretical maximum number of processes we can run at any given moments. If `cpu_count()` retuns `10`, in practice, you will never have `10` workers going at one time. Python turns out to be a polite program. If it sees other processes on running on the machine, it may only use `8` or `9` out of `10`. This is important for the next two lines of code.\n",
      "```python\n",
      "max_number_processes = 2\n",
      "pool = multiprocessing.Pool(max_number_processes)\n",
      "```\n",
      "In the first line, I set a maximum number of worker processes that multiprocessing should run at any given moment. Each worker is assigned its own process identification number (pid). You will notice that I stayed on the conservative side by picking `2`. However, Python will allow you to set the value to `cpu_count()` or even higher. Since Python will only run processes on available cores, setting `max_number_processes` to `20` on a `10` core machine will still mean that Python may only use `8` worker processes.\n",
      "```python\n",
      "total_tasks = 16\n",
      "tasks = range(total_tasks)\n",
      "results = pool.map_async(work, jobs)\n",
      "```\n",
      "OK, the next two lines are where the magic really begins to happen! In this case, we are using `map_async`. We will skip what this means for now but `map_async` assigns work to the worker processes. To define the 'work' done by the worker processes, we pass a `func` I've called `work` as the first argument in `map_async` and an `iterable`, which I've defined as `tasks` which is a `list` of integers using `range()`. `tasks` is any `iterable` Python object. It contains all the `args` you want to pass to `work`. I'll show a more complicated and more useful iterable later.\n",
      "\n",
      "It is important to note two things about what we've done. The first is that the number of tasks, 16, is greater than the theoretical maximum number of processes we can run. This means when a worker is done with 'one unit of work,' it moves on to the next. The next note is that `map_async` does the work in random order and does not wait for a preceeding task to finish before starting a new task. This is the fastest approach, but it means that `Unit of work number 6` may print before `Unit of work number 2`. In this case, we aren't worried about that.\n",
      "\n",
      "The last two lines of code I discovered through painful trial and error.\n",
      "```python\n",
      "pool.close()\n",
      "pool.join()\n",
      "```\n",
      "OK, so `close()` means that we cannot submit new tasks to our pool of worker processes. Once all the tasks have been completed the worker processes will exit. This is required before we can run what's probably the most important line in this example, `join()`. `join()` says that the code in `__main__` must wait until all our taks are complete before continuing! Why is this important? Well, if you plan to use the results returned by your worker processes, you must make the code pause. If you do not use `join`, the remainder of your script will run after `map_async` spawns, assigns the tasks, our worker processes. This means you may get errors and other parts of your code will throw a `NameError` or another ugly error.\n",
      "\n",
      "OK, so in this example, we didn't do anything to interesting besides introduce the basic structure of a multiproccessing Python script. Please note that you can always import a `func` you plan to use for your 'unit of work.'"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Measuring preformance gains\n",
      "Now that we know what `multiprocessing` is and how to write a Python script which ueses the package, how do we know whether or not we've gained anything? In parallel computing, there are a few metrics with which we can evaluate our performance. This is a great excerise in determining what the value of `max_number_processes`. On a quick sidenote, while Python maybe polite up to a certain extent, please make sure that if you are using a shared machine with multiple users, you do not completely take over the system.\n",
      "\n",
      "### Speedup\n",
      "[Speedup](http://en.wikipedia.org/wiki/Speedup) is a metric used to assess the relative performance improvement gained by executing one task versus another. In parallel processing, we say the task is all the work we'd like to get done. In our case, we are comparing serial execution (a single process) of all the work versus a task parallism version. Speedup is defined as\n",
      "$$\n",
      "    S \\equiv \\frac{T_{\\text{Old}}}{T_{\\text{New}}}\n",
      "$$\n",
      "where $S$ is speedup, $T_{\\text{Old}}$ is the time taken to execute the script without improvement (Serial), and $T_{\\text{New}}$ is the time taken to execute the script with the improvement (Parallel).\n",
      "\n",
      "### Efficiency\n",
      "Theoretically, speedup should be linear in that $S$ is equivalent to the number of processors. We would expect\n",
      "$$\n",
      "S_p = p\n",
      "$$\n",
      "where $p$ is the number of processors and $S_p$ is the speedup in parallel computing. In practice, we never see this type of speedup. So, we come up with a new metric that we'll call efficiency. We'll define efficiency as\n",
      "$$\n",
      "E \\equiv \\frac{S_p}{p}=\\frac{T_{\\text{Serial}}}{pT_{\\text{p}}}.\n",
      "$$\n",
      "\n",
      "In both cases, we are using the simpliest forms for the definitions of speedup and efficiency. Parallel code is typically a blend of serial and parallel. [Amdahl's law](http://en.wikipedia.org/wiki/Amdahl%27s_law) and [Gustafson's law](http://en.wikipedia.org/wiki/Gustafson%27s_law) are likely more accurate but for our purposes, we'll look at the relative improvements.\n",
      "\n",
      "### Our test\n",
      "Let's define a test in which we increase the number of worker processes from 1 to `cpu_count * 2 + 1`. We'll run each value for `max_number_processes` several times then average the time taken. From here, we will plot speedup and efficiency. Expect the results that I get to be vastly different than those on your system. **Please note that this script can take time to run.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file performance.py\n",
      "\n",
      "import multiprocessing  # the module we will be using for multiprocessing\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import time\n",
      "from itertools import repeat\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style(\"white\")\n",
      "\n",
      "\n",
      "def work(task):\n",
      "    \"\"\"\n",
      "    Some amount of work that will take time\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    task : tuple\n",
      "        Contains number, loop, and number processors\n",
      "    \"\"\"\n",
      "    number, loop = task\n",
      "    b = 2. * number - 1.\n",
      "    for i in range(loop):\n",
      "        a, b = b * i, number * i + b\n",
      "    return a, b\n",
      "\n",
      "def plot(multip_stats):\n",
      "    \"\"\"\n",
      "    plots times from multiprocessing\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    multip_stats : dictionary\n",
      "        dictionary containing time running\n",
      "    \"\"\"\n",
      "    serial_time = multip_stats[1].mean()\n",
      "    keys = multip_stats.keys()\n",
      "    keys.sort()\n",
      "    keys = np.array(keys)\n",
      "    speedup = []\n",
      "    efficiency = []\n",
      "    for number_processes in keys:\n",
      "        speedup.append(serial_time / multip_stats[number_processes].mean())\n",
      "        efficiency.append(speedup[-1] / number_processes)\n",
      "    fig = plt.figure(figsize=(6, 6))\n",
      "    ax = fig.add_subplot(211)\n",
      "    plt.plot(keys, keys)\n",
      "    plt.bar(keys-0.5, speedup, width=1)\n",
      "    plt.ylabel('Speedup')\n",
      "    ax.set_xticks(range(1, keys[-1] + 1))\n",
      "    ax.set_xticklabels([])\n",
      "    plt.xlim(0.5, keys[-1] + .5)\n",
      "    \n",
      "    ax = fig.add_subplot(212)\n",
      "    plt.bar(keys-0.5, efficiency, width=1)\n",
      "    plt.ylabel('Efficiency')\n",
      "    plt.xlabel('Number of processes')\n",
      "    ax.set_xticks(range(1, keys[-1] + 1))\n",
      "    ax.set_xticklabels(range(1, keys[-1] + 1))\n",
      "    plt.xlim(0.5, keys[-1] + .5)\n",
      "    plt.savefig(\"./parallel_speedup_efficiency.png\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    cpu_count = multiprocessing.cpu_count()\n",
      "    print(\"There are %d CPUs on this machine\" % cpu_count)\n",
      "    number_processes = range(1, cpu_count * 2 + 1)\n",
      "    loop = 1000\n",
      "    total_tasks = 1000\n",
      "    tasks = np.float_(range(1, total_tasks))\n",
      "    number_of_times_to_repeat = 20\n",
      "    multip_stats = {}\n",
      "    for number in number_processes:\n",
      "        multip_stats[number] = np.empty(number_of_times_to_repeat)\n",
      "        for i in range(number_of_times_to_repeat):\n",
      "            pool = multiprocessing.Pool(number)\n",
      "            start_time = time.time()\n",
      "            results = pool.map_async(work, zip(tasks, repeat(loop)))\n",
      "            pool.close()\n",
      "            pool.join()\n",
      "            end_time = time.time()\n",
      "            multip_stats[number][i] = end_time - start_time\n",
      "    plot(multip_stats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting performance.py\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "python performance.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 8 CPUs on this machine\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The figure below shows the speedup and efficiency improvements for the case above. It is always a good idea to conduct a similar test for your system and code with the goal of finding the optimal number of processes. From this example, we can see that our speedup maximizes around 9 processes. Why 9 when the machine has 8 CPUs? Since other processes can be running on the machine, we will never see an optimal speedup of $S_p=p$. Why were we allows to have more thay 8 processes when we only have 8 CPUs? As we mentioned earlier, we can have more workers than CPUs. However, the workers currently working is limited to the available CPUs. But, it is worth noting that while we maximized at 9, we never see our ideal/theoretical speedup. Efficiency paints a clearer picture of this metric.\n",
      "\n",
      "The actual results will very wildly based on the system and the other processes running on the system. \n",
      "\n",
      "Why is the plot so noisy? The \"noise\" you see in each bar is related the the `number_of_times_repeat`. The higher the number, the more likely we will see the true distribution on the machine. The variation will be cause by other processes running on the system and operating system level jobs running concurrently.\n",
      "\n",
      "![Speedup and parallel efficiency](https://cslocumwx.github.io/images/parallel_speedup_efficiency.png \"Speedup and parallel efficiency\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exploiting `multiprocessing` to get around a Python 2.x bug\n",
      "I recently ran into an issues with Python 2.x. Luckily, it isn't a problem in Python 3.x so it is probably time to upgrade. However, I run code on a number of operational machines which will likely not upgrade to Python 3.x for another 5-10 years. What is the issue? I notices that some of my scripts would hog a large amount of RAM in time. After a task within the script was completed and the next started, the RAM usage would double as if the RAM was never cleared. That's probably fine for small scripts but I crashed several systems which ran out of RAM. What was going on?!?! Whatever it was, it has to be stopped or I need to switch to another language (shudder).\n",
      "\n",
      "It turns out that Python does its job and correctly garbage collects after each task is complete. OK, that information isn't helpful. However, I use a number of packages like NumPy, SciPy, and matplotlib which wrap compiled C code. Apparently, it is a known issue that the C libraries receives Python's garbage collection signal but doesn't use it. Python thinks it did its job but C failed to clear the RAM. It is my understanding that the Python developers will not fix this for Python 2.x.\n",
      "\n",
      "OK, so what do we do! Here is the aha moment. The RAM is freed when the process ends (if you didn't crash the machine before that). What is we use `multiprocessing`? As I mentioned earlier, each worker process has its own process ID. When a worker finishes, it releases its RAM. However, each worker process could still take up more than its fair share of RAM. Never fear! A Python keyword argument is here! `multiprocessing.Pool` takes a keyword argument called `maxtasksperchild`. `maxtasksperchild` sets how many tasks a worker process is allowed to do. In order to skirt our issue, all we need to do is `maxtasksperchild=1`. OK, but wait, if we need to run 30 tasks and only set `max_number_process` to 2, don't we run out of workers? Luckily, no! `multiprocessing` only allows 2 workers to exist at any given time so Python keeps generating workers until the `close()` and `join()` are called.\n",
      "\n",
      "In the example below, I also included some memory profiling information."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file py2bug.py\n",
      "\n",
      "import multiprocessing \n",
      "import resource\n",
      "from itertools import repeat\n",
      "import numpy as np\n",
      "import sys\n",
      "\n",
      "def work(task):\n",
      "    \"\"\"\n",
      "    Some amount of work that will take time\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    task : tuple\n",
      "        Contains number, loop, and number processors\n",
      "    \"\"\"\n",
      "    number, loop = task\n",
      "    b = 2. * number - 1.\n",
      "    empty_array = np.ones((loop, loop))\n",
      "    for i in range(loop):\n",
      "        a, b = b * i, number * i + b\n",
      "    print('\\tWorker maximum memory usage: %.2f (mb)' % (current_mem_usage()))\n",
      "    return a, b, empty_array\n",
      "\n",
      "def current_mem_usage():\n",
      "    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024.\n",
      "    \n",
      "if __name__ == \"__main__\":\n",
      "    cpu_count = multiprocessing.cpu_count()\n",
      "    print(\"There are %d CPUs on this machine\" % cpu_count)\n",
      "    loop = 1000\n",
      "    total_tasks = 15\n",
      "    tasks = np.float_(range(1, total_tasks))\n",
      "    \n",
      "    print(\"Same worker for all tasks:\")\n",
      "    pool = multiprocessing.Pool(processes=1, maxtasksperchild=total_tasks)\n",
      "    results = pool.map_async(work, zip(tasks, repeat(loop)))\n",
      "    pool.close()\n",
      "    pool.join()\n",
      "    del pool\n",
      "    print('Global maximum memory usage: %.2f (mb)' % current_mem_usage())\n",
      "    \n",
      "    print(\"Respawn worker for each task:\")\n",
      "    pool = multiprocessing.Pool(processes=1, maxtasksperchild=1)\n",
      "    results = pool.map_async(work, zip(tasks, repeat(loop)))\n",
      "    pool.close()\n",
      "    pool.join()\n",
      "    del pool\n",
      "    print('Global maximum memory usage: %.2f (mb)' % current_mem_usage())\n",
      "    \n",
      "    print(\"Use a Python loop:\")\n",
      "    mem_usage = current_mem_usage()\n",
      "    for task in zip(tasks, repeat(loop)):\n",
      "        a, b, empty_array = work(task)\n",
      "    print('Global maximum memory usage: %.2f (mb)' % current_mem_usage())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting py2bug.py\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "python py2bug.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 8 CPUs on this machine\n",
        "Same worker for all tasks:\n",
        "\tWorker maximum memory usage: 19.83 (mb)\n",
        "\tWorker maximum memory usage: 27.57 (mb)\n",
        "\tWorker maximum memory usage: 35.20 (mb)\n",
        "\tWorker maximum memory usage: 42.83 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 111.61 (mb)\n",
        "\tWorker maximum memory usage: 134.49 (mb)\n",
        "\tWorker maximum memory usage: 134.49 (mb)\n",
        "Global maximum memory usage: 160.98 (mb)\n",
        "Respawn worker for each task:\n",
        "\tWorker maximum memory usage: 157.17 (mb)\n",
        "\tWorker maximum memory usage: 157.25 (mb)\n",
        "\tWorker maximum memory usage: 157.25 (mb)\n",
        "\tWorker maximum memory usage: 157.25 (mb)\n",
        "\tWorker maximum memory usage: 73.39 (mb)\n",
        "\tWorker maximum memory usage: 73.48 (mb)\n",
        "\tWorker maximum memory usage: 73.48 (mb)\n",
        "\tWorker maximum memory usage: 80.98 (mb)\n",
        "\tWorker maximum memory usage: 80.93 (mb)\n",
        "\tWorker maximum memory usage: 88.64 (mb)\n",
        "\tWorker maximum memory usage: 96.27 (mb)\n",
        "\tWorker maximum memory usage: 103.90 (mb)\n",
        "\tWorker maximum memory usage: 141.94 (mb)\n",
        "\tWorker maximum memory usage: 142.02 (mb)\n",
        "Global maximum memory usage: 161.00 (mb)\n",
        "Use a Python loop:\n",
        "\tWorker maximum memory usage: 161.00 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "\tWorker maximum memory usage: 161.04 (mb)\n",
        "Global maximum memory usage: 161.04 (mb)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example, you can see that the serial case slowly grows in time. This isn't a very dramatic difference but we aren't doing much in our work function. You can imagine that in a bigger program how repetition could cause problems. Another thing to note is that just using 'multiprocessing' helps eliminate some of the issues with memory since the main program is isolated from the worker as a seperate process as the example shows. In both of the 'multiprocessing' instances, memory usage fluctuates. Respawning potentially uses more memory per task since the CPU must continually reallocate RAM. In reality, you must fine the best method for your problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Concluding remarks\n",
      "\n",
      "In this post, we have explored the task parallelism option available in the standard library of Python. We have shown how using task parallelism speeds up code in human time even if it isn't the most efficient usage of the cores. We also explored how task parallelism can be used to avoid the Python 2.x memory bug."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This post was written using an IPython notebook.  You can\n",
      "[download](http://cslocumwx.github.io/downloads/notebooks/PythonMultiprocessing.ipynb)\n",
      "this notebook, or see a static view\n",
      "[here](http://nbviewer.ipython.org/url/cslocum.github.io/downloads/notebooks/PythonMultiprocessing.ipynb).\n",
      "\n",
      "*Please note that <span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Multiprocessing: Task parallelism for the masses</span> by <a\n",
      "        xmlns:cc=\"http://creativecommons.org/ns#\"\n",
      "        href=\"http://schubert.atmos.colostate.edu/~cslocum/\"\n",
      "        property=\"cc:attributionName\" rel=\"cc:attributionURL\">Chris Slocum</a>\n",
      "    is licensed under a <a rel=\"license\"\n",
      "        href=\"http://creativecommons.org/licenses/by-nc-nd/3.0/deed.en_US\">Creative\n",
      "        Commons Attribution-NonCommercial-NoDerivs 3.0 Unported License</a>.*\n",
      "</small>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}